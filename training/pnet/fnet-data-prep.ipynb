{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n    break\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n\nimport json \nimport cv2\n\nimport pandas as pd ","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2021-10-18T10:02:23.767580Z","iopub.execute_input":"2021-10-18T10:02:23.767887Z","iopub.status.idle":"2021-10-18T10:02:23.776140Z","shell.execute_reply.started":"2021-10-18T10:02:23.767849Z","shell.execute_reply":"2021-10-18T10:02:23.775103Z"},"trusted":true},"execution_count":141,"outputs":[]},{"cell_type":"code","source":"img=cv2.imread(\"../input/wider-face-recognization/WIDER_train/WIDER_train/images/0--Parade/0_Parade_marchingband_1_799.jpg\")","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:23.778187Z","iopub.execute_input":"2021-10-18T10:02:23.778456Z","iopub.status.idle":"2021-10-18T10:02:23.824904Z","shell.execute_reply.started":"2021-10-18T10:02:23.778427Z","shell.execute_reply":"2021-10-18T10:02:23.823690Z"},"trusted":true},"execution_count":142,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt \n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:23.826583Z","iopub.execute_input":"2021-10-18T10:02:23.826865Z","iopub.status.idle":"2021-10-18T10:02:23.833890Z","shell.execute_reply.started":"2021-10-18T10:02:23.826809Z","shell.execute_reply":"2021-10-18T10:02:23.833079Z"},"trusted":true},"execution_count":143,"outputs":[]},{"cell_type":"code","source":"plt.imshow(cv2.cvtColor(img,cv2.COLOR_BGR2RGB))","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:23.834962Z","iopub.execute_input":"2021-10-18T10:02:23.835208Z","iopub.status.idle":"2021-10-18T10:02:24.206525Z","shell.execute_reply.started":"2021-10-18T10:02:23.835179Z","shell.execute_reply":"2021-10-18T10:02:24.205444Z"},"trusted":true},"execution_count":144,"outputs":[]},{"cell_type":"code","source":"d=open('../input/wider-face-recognization/wider_face_split/wider_face_split/wider_face_train_bbx_gt.txt','r').read()\nd= d.split('\\n')\nfiles= {}\nx=0\ncur_index=0\nwhile x<(len(d)-1) :    \n    if d[x].find('.jpg')>-1:\n        files[d[x]]={}\n        files[d[x]]['num_faces']=d[x+1]\n        files[d[x]]['bbox']=[]\n\n        #print(d[x])\n        cur_index=x+2\n        while True:\n            try:\n                if d[cur_index].find('.jpg')>-1:\n                    break\n            except:\n                print(f\"Catch {cur_index} completed\")\n                break\n            try:\n                files[d[x]]['bbox'].append([int(x) for x in  d[cur_index].strip().split(' ') ] )\n            except:\n                print(f\"This threw error {cur_index} {d[cur_index]}\")\n                pass\n            cur_index+=1\n        x=cur_index\n\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:24.209248Z","iopub.execute_input":"2021-10-18T10:02:24.209580Z","iopub.status.idle":"2021-10-18T10:02:25.117476Z","shell.execute_reply.started":"2021-10-18T10:02:24.209528Z","shell.execute_reply":"2021-10-18T10:02:25.116503Z"},"trusted":true},"execution_count":145,"outputs":[]},{"cell_type":"code","source":"train_df=pd.DataFrame(files.items())\ntrain_df[train_df.iloc[:,1].apply(lambda x : x['num_faces'])=='1']","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:25.118709Z","iopub.execute_input":"2021-10-18T10:02:25.118970Z","iopub.status.idle":"2021-10-18T10:02:26.049522Z","shell.execute_reply.started":"2021-10-18T10:02:25.118940Z","shell.execute_reply":"2021-10-18T10:02:26.044195Z"}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def transformCoordinates(bb):\n    bb=bb['bbox']\n    new_bb=[]\n    for x in bb:\n        new_bb.append(np.array([x[0],x[1],x[0]+x[2],x[1]+x[3]]))\n    return np.array(new_bb)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.050564Z","iopub.status.idle":"2021-10-18T10:02:26.050966Z","shell.execute_reply.started":"2021-10-18T10:02:26.050746Z","shell.execute_reply":"2021-10-18T10:02:26.050762Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.052378Z","iopub.status.idle":"2021-10-18T10:02:26.052712Z","shell.execute_reply.started":"2021-10-18T10:02:26.052544Z","shell.execute_reply":"2021-10-18T10:02:26.052562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['bb_trans']=train_df.iloc[:,1].apply(transformCoordinates)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.053871Z","iopub.status.idle":"2021-10-18T10:02:26.054202Z","shell.execute_reply.started":"2021-10-18T10:02:26.054030Z","shell.execute_reply":"2021-10-18T10:02:26.054045Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.055170Z","iopub.status.idle":"2021-10-18T10:02:26.055487Z","shell.execute_reply.started":"2021-10-18T10:02:26.055307Z","shell.execute_reply":"2021-10-18T10:02:26.055321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ind=12875\nimg=cv2.imread(f'../input/wider-face-recognization/WIDER_train/WIDER_train/images/{train_df.iloc[ind,0]}',cv2.COLOR_BGR2RGB)\nimg_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\nplt.imshow(img_rgb )\n\nfor x in train_df.loc[ind,'bb_trans']:\n    print(x)\n    cv2.rectangle(img_rgb,(x[0],x[1]),(x[2],x[3]),(40,125,200),2)\n    print(x[:4])\nplt.figure(figsize = (14.8,10.8))\n\nplt.imshow(img_rgb)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.056244Z","iopub.status.idle":"2021-10-18T10:02:26.056539Z","shell.execute_reply.started":"2021-10-18T10:02:26.056382Z","shell.execute_reply":"2021-10-18T10:02:26.056396Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\ndef IoU(box, boxes):\n    \"\"\"Compute IoU between detect box and gt boxes\n    Parameters:\n    ----------\n    box: numpy array , shape (5, ): x1, y1, x2, y2, score \n        input box\n    boxes: numpy array, shape (n, 4): x1, y1, x2, y2\n        input ground truth boxes\n    Returns:\n    -------\n    ovr: numpy.array, shape (n, )         IoU\n    \"\"\"\n    # box = (x1, y1, x2, y2)\n    \n    box_area = (box[2] - box[0] + 1) * (box[3] - box[1] + 1)\n    area = (boxes[:, 2] - boxes[:, 0] + 1) * (boxes[:, 3] - boxes[:, 1] + 1)\n    # abtain the offset of the interception of union between crop_box and gt_box\n    xx1 = np.maximum(box[0], boxes[:, 0])\n    yy1 = np.maximum(box[1], boxes[:, 1])\n    xx2 = np.minimum(box[2], boxes[:, 2])\n    yy2 = np.minimum(box[3], boxes[:, 3])\n    w = np.maximum(0, xx2 - xx1 + 1)\n    h = np.maximum(0, yy2 - yy1 + 1)\n    inter = w * h\n    ovr = inter / (box_area + area - inter)\n    return ovr","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.057403Z","iopub.status.idle":"2021-10-18T10:02:26.057865Z","shell.execute_reply.started":"2021-10-18T10:02:26.057655Z","shell.execute_reply":"2021-10-18T10:02:26.057674Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.loc[856,'bb_trans']","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.059022Z","iopub.status.idle":"2021-10-18T10:02:26.059328Z","shell.execute_reply.started":"2021-10-18T10:02:26.059170Z","shell.execute_reply":"2021-10-18T10:02:26.059185Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"IoU([490, 260, 500, 362],train_df.loc[856,'bb_trans'])","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.060326Z","iopub.status.idle":"2021-10-18T10:02:26.060637Z","shell.execute_reply.started":"2021-10-18T10:02:26.060469Z","shell.execute_reply":"2021-10-18T10:02:26.060484Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn as nn","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.061908Z","iopub.status.idle":"2021-10-18T10:02:26.064010Z","shell.execute_reply.started":"2021-10-18T10:02:26.063691Z","shell.execute_reply":"2021-10-18T10:02:26.063718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PnetMy(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.cov1 = nn.Conv2d( in_channels =3,out_channels= 10,kernel_size =(3,3),stride= 1)\n        self.prelu1 = nn.PReLU(10)\n        self.maxPool1 = nn.MaxPool2d(kernel_size=(3,3) ,stride=2,ceil_mode=True)\n        self.cov2=nn.Conv2d( in_channels= 10,out_channels= 16,kernel_size= (3,3),stride= 1)\n        self.prelu2 = nn.PReLU(16)\n        self.cov3=nn.Conv2d( in_channels= 16,out_channels= 32,kernel_size= (3,3),stride= 1)\n        self.prelu3 = nn.PReLU(32)\n        self.face_detector=nn.Conv2d( in_channels= 32,out_channels= 1,kernel_size= (1,1),stride= 1)\n        self.softmax = nn.Softmax(dim=1)\n        self.bb_regression=nn.Conv2d( in_channels= 32,out_channels= 4,kernel_size= (1,1),stride= 1)\n        self.facial_landmark=nn.Conv2d( in_channels= 32,out_channels= 10,kernel_size= (1,1),stride= 1)\n    def forward(self,x):\n        x=self.cov1(x)\n        x=self.prelu1(x)\n        #print(f\"Shape should be n*10*10*10 {x.shape}\")\n        x=self.maxPool1(x)\n        #print(f\"Shape after MaxPool should be n*10*5*5 {x.shape}\")\n        x=self.cov2(x)\n        x=self.prelu2(x)\n        #print(f\"Shape should be n*16*3*3 {x.shape}\")\n        x=self.cov3(x)\n        x=self.prelu3(x)\n        #print(f\"Shape should be n*32*1*1 {x.shape}\")\n        is_face=self.face_detector(x)\n        is_face=is_face.view((-1,1))\n        #is_face=self.softmax(is_face)\n        bb_output=self.bb_regression(x)\n        bb_output=bb_output.view((-1,4))\n        return is_face,bb_output","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.065200Z","iopub.status.idle":"2021-10-18T10:02:26.066292Z","shell.execute_reply.started":"2021-10-18T10:02:26.065998Z","shell.execute_reply":"2021-10-18T10:02:26.066025Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch.nn.functional as F","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.067399Z","iopub.status.idle":"2021-10-18T10:02:26.068530Z","shell.execute_reply.started":"2021-10-18T10:02:26.068234Z","shell.execute_reply":"2021-10-18T10:02:26.068262Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.069627Z","iopub.status.idle":"2021-10-18T10:02:26.070536Z","shell.execute_reply.started":"2021-10-18T10:02:26.070237Z","shell.execute_reply":"2021-10-18T10:02:26.070267Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mypnet=PnetMy()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.071822Z","iopub.status.idle":"2021-10-18T10:02:26.072342Z","shell.execute_reply.started":"2021-10-18T10:02:26.072069Z","shell.execute_reply":"2021-10-18T10:02:26.072103Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dim_input = torch.randn( 2,3, 500,500)\nis_face,bb=mypnet(dim_input)\ncheck1= nn.Conv2d(in_channels=3,out_channels=4,kernel_size=12,stride=2,padding=0)(dim_input)\ncheck1.shape,bb.shape , is_face.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.073902Z","iopub.status.idle":"2021-10-18T10:02:26.074688Z","shell.execute_reply.started":"2021-10-18T10:02:26.074406Z","shell.execute_reply":"2021-10-18T10:02:26.074434Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!mkdir ./pnet\n!mkdir ./pnet/train\n\n\n!mkdir ./pnet/train/neg\n!mkdir ./pnet/train/pos\n!rm ./pnet/train/neg -r\n!rm ./pnet/train/pos -r\n\n!mkdir ./pnet/train/neg\n!mkdir ./pnet/train/pos","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.076101Z","iopub.status.idle":"2021-10-18T10:02:26.076420Z","shell.execute_reply.started":"2021-10-18T10:02:26.076258Z","shell.execute_reply":"2021-10-18T10:02:26.076273Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!tree ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.077264Z","iopub.status.idle":"2021-10-18T10:02:26.077561Z","shell.execute_reply.started":"2021-10-18T10:02:26.077403Z","shell.execute_reply":"2021-10-18T10:02:26.077418Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nimport cv2\nimport uuid\nimport time\np_idx=0\nn_idx=0\nd_idx=0\nidx=1\nbox_idx=0\nneg_directory='./pnet/train/neg'\npos_directory='./pnet/train/pos'\nlabels={}\norig=time.time()\ntick=orig\nfor x in tqdm(train_df.iterrows(),total=train_df.shape[0]):\n    try:\n        #print(x[1][0])\n        file_path=f'../input/wider-face-recognization/WIDER_train/WIDER_train/images/{x[1][0]}'\n        bbox=x[1]['bb_trans']\n        #print(bbox)\n        img=cv2.imread(file_path,cv2.COLOR_BGR2RGB)\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        height, width, channel = img.shape\n        #print(img.shape)\n        idx+=1\n        if idx%250 ==0 :\n            last_tick=tick\n            tick=time.time()\n\n            print(f\"Completed {idx} images in {tick-last_tick}   total_time_passed {tick-orig}  average : {(tick-last_tick)/250}\")\n            print(\"%s images done, pos: %s part: %s neg: %s\" % (idx, p_idx, d_idx, n_idx))\n\n        neg_num = 0\n        max_retires=25\n        current_retry=0\n        while neg_num < 10 and current_retry< max_retires:\n            current_retry+=1\n            size = np.random.randint(12, min(width, height) / 2)\n            nx = np.random.randint(0, width - size)\n            ny = np.random.randint(0, height - size)\n            crop_box = np.array([nx, ny, nx + size, ny + size])\n\n            Iou = IoU(crop_box, bbox)\n\n            cropped_im = img[ny: ny + size, nx: nx + size, :]\n            resized_im = cv2.resize(cropped_im, (12, 12), interpolation=cv2.INTER_LINEAR)\n            filename=str(uuid.uuid4())\n            if np.max(Iou) < 0.3:\n                # Iou with all gts must below 0.3\n                save_file=f\"{neg_directory}/{filename}_0_nonoverlap.jpg\"\n                labels[f\"{filename}_0_nonoverlap.jpg\"]={'label':0,'bb':[]}\n                cv2.imwrite(save_file, resized_im)\n                n_idx += 1\n                neg_num += 1\n        for box in bbox:\n            # box (x_left, y_top, x_right, y_bottom)\n            x1, y1, x2, y2 = box\n            # w = x2 - x1 + 1\n            # h = y2 - y1 + 1\n            w = x2 - x1 + 1\n            h = y2 - y1 + 1\n\n            # ignore small faces\n            # in case the ground truth boxes of small faces are not accurate\n            if max(w, h) < 40 or x1 < 0 or y1 < 0:\n                continue\n\n                    # generate negative examples that have overlap with gt\n            for i in range(5):\n                size = np.random.randint(12, min(width, height) / 2)\n                # delta_x and delta_y are offsets of (x1, y1)\n\n                delta_x = np.random.randint(max(-size, -x1), w)\n                delta_y = np.random.randint(max(-size, -y1), h)\n                nx1 = max(0, x1 + delta_x)\n                ny1 = max(0, y1 + delta_y)\n\n                if nx1 + size > width or ny1 + size > height:\n                    continue\n                crop_box = np.array([nx1, ny1, nx1 + size, ny1 + size])\n                Iou = IoU(crop_box, bbox)\n\n                cropped_im = img[ny1: ny1 + size, nx1: nx1 + size, :]\n                resized_im = cv2.resize(cropped_im, (12, 12), interpolation=cv2.INTER_LINEAR)\n                filename=str(uuid.uuid4())\n                if np.max(Iou) < 0.3:\n                    # Iou with all gts must below 0.3\n                    save_file=f\"{neg_directory}/{filename}_0_overlap.jpg\"\n                    #f2.write(save_file + ' 0\\n')\n                    labels[f\"{filename}_0_overlap.jpg\"]={'label':0,'bb':[]}\n                    cv2.imwrite(save_file, resized_im)\n                    n_idx += 1\n            for i in range(8):\n                size = np.random.randint(int(min(w, h) * 0.8), np.ceil(1.25 * max(w, h)))\n\n                # delta here is the offset of box center\n                delta_x = np.random.randint(-w * 0.15, w * 0.15)\n                delta_y = np.random.randint(-h * 0.15, h * 0.15)\n\n                nx1 = max(x1 + w / 2 + delta_x - size / 2, 0)\n                ny1 = max(y1 + h / 2 + delta_y - size / 2, 0)\n                nx2 = nx1 + size\n                ny2 = ny1 + size\n\n                if nx2 > width or ny2 > height:\n                    continue\n                crop_box = np.array([nx1, ny1, nx2, ny2])\n\n                offset_x1 = (x1 - nx1) / float(size)\n                offset_y1 = (y1 - ny1) / float(size)\n                offset_x2 = (x2 - nx2) / float(size)\n                offset_y2 = (y2 - ny2) / float(size)\n\n                cropped_im = img[int(ny1): int(ny2), int(nx1): int(nx2), :]\n                resized_im = cv2.resize(cropped_im, (12, 12), interpolation=cv2.INTER_LINEAR)\n\n                box_ = box.reshape(1, -1)\n                filename=str(uuid.uuid4())\n                if IoU(crop_box, box_) >= 0.70:\n\n                    save_file=f\"{pos_directory}/{filename}_1_pos.jpg\"\n                    labels[f\"{filename}_1_pos.jpg\"]={'label':1,'bb':     [offset_x1, offset_y1, offset_x2, offset_y2]}\n                    cv2.imwrite(save_file, resized_im)\n                    p_idx += 1\n                elif IoU(crop_box, box_) >= 0.4:\n                    save_file=f\"{pos_directory}/{filename}_1_part.jpg\"\n                    labels[f\"{filename}_1_part.jpg\"]={'label':-1,'bb':     [offset_x1, offset_y1, offset_x2, offset_y2]}\n                    cv2.imwrite(save_file, resized_im)\n                    d_idx += 1\n\n            box_idx += 1\n    except Exception as e:\n        #raise e \n        print(\"Encountered exception \")\n        pass\n        \n        \nimport pickle as pkl\n\n#pkl.dump(labels,open(\"annotations.pkl\",'wb'))\nimport json \njson.dump(labels, open(\"annotations.json\",\"w\"), indent = 1)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.078912Z","iopub.status.idle":"2021-10-18T10:02:26.079621Z","shell.execute_reply.started":"2021-10-18T10:02:26.079402Z","shell.execute_reply":"2021-10-18T10:02:26.079427Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getfilename(x,base_directory=\"./pnet/train\"):\n    neg_directory=\"neg\"\n    pos_directory=\"pos\"\n    if int(x.split('_')[-2])==0:\n        return f\"{base_directory}/{neg_directory}/{x}\"\n    else:\n        return f\"{base_directory}/{pos_directory}/{x}\"\n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.080463Z","iopub.status.idle":"2021-10-18T10:02:26.081140Z","shell.execute_reply.started":"2021-10-18T10:02:26.080918Z","shell.execute_reply":"2021-10-18T10:02:26.080942Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.082091Z","iopub.status.idle":"2021-10-18T10:02:26.082842Z","shell.execute_reply.started":"2021-10-18T10:02:26.082600Z","shell.execute_reply":"2021-10-18T10:02:26.082626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from __future__ import print_function, division\nimport os\nimport torch\nimport pandas as pd\nfrom skimage import io, transform\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms, utils\nclass PNetDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n    def __init__(self, data,file_ids, directory, transform=None):\n        self.all_files=data\n        self.file_ids=file_ids\n        self.directory=directory\n        self.transform=transform\n    def __len__(self):\n        return len(self.file_ids)\n\n    def __getitem__(self, idx):\n        \n        img_name = getfilename(self.file_ids[idx])\n        #print(f\"{img_name}\\n\")\n        labels= self.all_files[self.file_ids[idx]]['label']\n        if labels!=0:\n            labels=1\n        bb = np.array(self.all_files[self.file_ids[idx]]['bb'],dtype=float)\n        if len(bb)==0:\n            bb=np.array([0,0,0,0],dtype=float)\n        \n        image=cv2.imread(img_name)\n        image=image/255\n        image=image.astype(np.float32)\n        \n        \n        return {'image':torch.from_numpy(image).permute(2, 0, 1),'label':float(labels),'bb':torch.from_numpy(bb)}    \n        ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.083698Z","iopub.status.idle":"2021-10-18T10:02:26.084380Z","shell.execute_reply.started":"2021-10-18T10:02:26.084200Z","shell.execute_reply":"2021-10-18T10:02:26.084219Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"myPnetDataset=PNetDataset(data=labels,file_ids= list(labels.keys()),directory='./pnet/train')\n#trainDataloader = DataLoader(myPnetDataset,batch_size=32 ,num_workers=3, )","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.085420Z","iopub.status.idle":"2021-10-18T10:02:26.085732Z","shell.execute_reply.started":"2021-10-18T10:02:26.085569Z","shell.execute_reply":"2021-10-18T10:02:26.085584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mypnet=PnetMy()\nmypnet.train()\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.086631Z","iopub.status.idle":"2021-10-18T10:02:26.086978Z","shell.execute_reply.started":"2021-10-18T10:02:26.086778Z","shell.execute_reply":"2021-10-18T10:02:26.086801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"loss1 =torch.nn.BCEWithLogitsLoss()\nloss2 =torch.nn.MSELoss()\noptimizer = torch.optim.Adam(mypnet.parameters(), lr=0.02)\nif torch.cuda.is_available():  \n    dev = \"cuda:0\" \nelse:  \n    dev = \"cpu\"  \ndevice = torch.device(dev)\nmypnet.to(device)\nbatch_size=2048\ntrainDataloader = DataLoader(myPnetDataset,batch_size=batch_size ,num_workers=4, )\nfor epoch in range(0,5):\n    cls_loss_sum=0\n    bb_loss_sum=0\n    for batch_id, data in enumerate(trainDataloader):\n        images=data['image'].float().to(device)\n        gt_bbox=data['bb'].float().to(device)\n        gt_isface=data['label'].float().to(device)\n        is_face, bb= mypnet(images)\n        cls_loss=loss1(is_face.float(),gt_isface.reshape(gt_isface.shape[0],1).float())\n        bb_loss=loss2(bb.float(), gt_bbox.float())\n        cls_loss_sum=cls_loss+cls_loss_sum\n        bb_loss_sum=bb_loss+bb_loss_sum\n        \n        loss_all= cls_loss+bb_loss\n        optimizer.zero_grad()\n        loss_all.backward()\n        optimizer.step()\n        if batch_id%80==0:\n            print(f\"{batch_id} {bb_loss_sum/((batch_id+1)*1)} {cls_loss_sum/((batch_id+1)*1)}\")\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.088522Z","iopub.status.idle":"2021-10-18T10:02:26.089178Z","shell.execute_reply.started":"2021-10-18T10:02:26.088956Z","shell.execute_reply":"2021-10-18T10:02:26.088979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(0,2):\n    cls_loss_sum=0\n    bb_loss_sum=0\n    for batch_id, data in enumerate(trainDataloader):\n        images=data['image'].float().to(device)\n        gt_bbox=data['bb'].float().to(device)\n        gt_isface=data['label'].float().to(device)\n        is_face, bb= mypnet(images)\n        cls_loss=loss1(is_face.float(),gt_isface.reshape(gt_isface.shape[0],1).float())\n        bb_loss=loss2(bb.float(), gt_bbox.float())\n        cls_loss_sum=cls_loss+cls_loss_sum\n        bb_loss_sum=bb_loss+bb_loss_sum\n        \n        loss_all= cls_loss+bb_loss\n        optimizer.zero_grad()\n        loss_all.backward()\n        optimizer.step()\n        if batch_id%20==0:\n            print(f\"{batch_id} {bb_loss_sum/((batch_id+1))} {cls_loss_sum/((batch_id+1))}\")\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.090079Z","iopub.status.idle":"2021-10-18T10:02:26.090409Z","shell.execute_reply.started":"2021-10-18T10:02:26.090230Z","shell.execute_reply":"2021-10-18T10:02:26.090252Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### scoring on the training set ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.091894Z","iopub.status.idle":"2021-10-18T10:02:26.092270Z","shell.execute_reply.started":"2021-10-18T10:02:26.092087Z","shell.execute_reply":"2021-10-18T10:02:26.092110Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"scores=0\nfrom sklearn.metrics import roc_auc_score\nfor index , K in enumerate(trainDataloader) :\n    is_face, bb= mypnet(K['image'].to(device))\n    scores+=roc_auc_score(K['label'],nn.Sigmoid()(is_face).cpu().detach().numpy())\n    if index%50==0:\n        print(f\"Target Label % {K['label'].sum()/K['label'].shape[0]} ,Batch ROC-AUC {(roc_auc_score(K['label'],nn.Sigmoid()(is_face).cpu().detach().numpy()) )} ,Overall ROC-AUC {scores/(index+1)}\" )","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.093474Z","iopub.status.idle":"2021-10-18T10:02:26.093819Z","shell.execute_reply.started":"2021-10-18T10:02:26.093638Z","shell.execute_reply":"2021-10-18T10:02:26.093661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"torch.save(mypnet.state_dict(), os.path.join('./',\"pnet_epoch_.pt\"))\ntorch.save(mypnet, os.path.join('./',\"pnet_epoch_model.pkl\"))","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.094889Z","iopub.status.idle":"2021-10-18T10:02:26.095224Z","shell.execute_reply.started":"2021-10-18T10:02:26.095047Z","shell.execute_reply":"2021-10-18T10:02:26.095069Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef resize_image( img, scale):\n    \"\"\"\n        resize image and transform dimention to [batchsize, channel, height, width]\n    Parameters:\n    ----------\n        img: numpy array , height x width x channel\n            input image, channels in BGR order here\n        scale: float number\n            scale factor of resize operation\n    Returns:\n    -------\n        transformed image tensor , 1 x channel x height x width\n    \"\"\"\n    height, width, channels = img.shape\n    new_height = int(height * scale)     # resized new height\n    new_width = int(width * scale)       # resized new width\n    new_dim = (new_width, new_height)\n    img_resized = cv2.resize(img, new_dim, interpolation=cv2.INTER_LINEAR)      # resized image\n    return img_resized","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.097996Z","iopub.status.idle":"2021-10-18T10:02:26.098586Z","shell.execute_reply.started":"2021-10-18T10:02:26.098364Z","shell.execute_reply":"2021-10-18T10:02:26.098391Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def changetotensor(img):\n    \"\"\"\n    convert H,W,C numpy image to C,H,W tensor\n    \"\"\"\n    #img=cv2.resize(img,(12,12))\n    ten= torch.from_numpy(img).float().permute(2, 0, 1)\n    output=torch.empty(size=(1,3,ten.shape[1],ten.shape[2]))\n    output[0]=ten/255\n    print(output.shape)\n    return output","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.099606Z","iopub.status.idle":"2021-10-18T10:02:26.100518Z","shell.execute_reply.started":"2021-10-18T10:02:26.100297Z","shell.execute_reply":"2021-10-18T10:02:26.100320Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"list(mypnet.children())","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.101500Z","iopub.status.idle":"2021-10-18T10:02:26.101918Z","shell.execute_reply.started":"2021-10-18T10:02:26.101679Z","shell.execute_reply":"2021-10-18T10:02:26.101703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"   \nimport torchvision.transforms as transforms\nimport torch\nfrom torch.autograd.variable import Variable\nimport numpy as np\n\ntransform = transforms.ToTensor()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.103392Z","iopub.status.idle":"2021-10-18T10:02:26.103747Z","shell.execute_reply.started":"2021-10-18T10:02:26.103555Z","shell.execute_reply":"2021-10-18T10:02:26.103577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def myOutputBb(mypnet,k):\n    return mypnet.bb_regression(mypnet.prelu3(mypnet.cov3(mypnet.prelu2(mypnet.cov2(mypnet.maxPool1(mypnet.prelu1((mypnet.cov1(k)))))))))\ndef myOutputcls(mypnet,k):\n    return mypnet.face_detector(mypnet.prelu3(mypnet.cov3(mypnet.prelu2(mypnet.cov2(mypnet.maxPool1(mypnet.prelu1((mypnet.cov1(k)))))))))\n    \n     \n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.105293Z","iopub.status.idle":"2021-10-18T10:02:26.105711Z","shell.execute_reply.started":"2021-10-18T10:02:26.105471Z","shell.execute_reply":"2021-10-18T10:02:26.105497Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_boxes = list()\ni = 0\nind=12875\n#ind=542\nim=cv2.imread(f'../input/wider-face-recognization/WIDER_train/WIDER_train/images/{train_df.iloc[ind,0]}',cv2.COLOR_BGR2RGB)\nim=cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\nplt.imshow(im)\nh, w, c = im.shape\n\nnet_size = 12\n\ncurrent_scale = float(net_size) / 350# find initial scale\n# print('imgshape:{0}, current_scale:{1}'.format(im.shape, current_scale))\nim_resized = resize_image(im, current_scale) # scale = 1.0\ncurrent_height, current_width, _ = im_resized.shape\n\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.106761Z","iopub.status.idle":"2021-10-18T10:02:26.107107Z","shell.execute_reply.started":"2021-10-18T10:02:26.106936Z","shell.execute_reply":"2021-10-18T10:02:26.106952Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"im_resized.shape","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.108180Z","iopub.status.idle":"2021-10-18T10:02:26.108491Z","shell.execute_reply.started":"2021-10-18T10:02:26.108330Z","shell.execute_reply":"2021-10-18T10:02:26.108345Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.imshow(im)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.109671Z","iopub.status.idle":"2021-10-18T10:02:26.110044Z","shell.execute_reply.started":"2021-10-18T10:02:26.109848Z","shell.execute_reply":"2021-10-18T10:02:26.109873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math \nimg=cv2.resize(im,(50,50)) #### only squares \nhas_face,face_activated_bb= mypnet(changetotensor(img).to(device))\nface_activated_bb.shape\n\nre_im =int(math.sqrt(face_activated_bb.shape[0]))\nhas_face.shape,bb.shape\nplt.imshow(F.sigmoid(has_face).view((re_im,re_im)).detach().cpu().numpy())\nplt.show()\nplt.imshow(img)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.110888Z","iopub.status.idle":"2021-10-18T10:02:26.111211Z","shell.execute_reply.started":"2021-10-18T10:02:26.111047Z","shell.execute_reply":"2021-10-18T10:02:26.111064Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math \nimg=cv2.resize(im,(150,250)) #### any resize  \nhas_face=myOutputcls(mypnet,changetotensor(img).to(device))\nprint(has_face.shape)\nplt.imshow(F.sigmoid(has_face[0,0,:,:]).detach().cpu().numpy())\nplt.show()\nplt.imshow(img)\n","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:02:26.112229Z","iopub.status.idle":"2021-10-18T10:02:26.112559Z","shell.execute_reply.started":"2021-10-18T10:02:26.112382Z","shell.execute_reply":"2021-10-18T10:02:26.112398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"all_boxes = list()\ni = 0\nind=12875\n#ind=542\nim=cv2.imread(f'../input/wider-face-recognization/WIDER_train/WIDER_train/images/{train_df.iloc[ind,0]}',cv2.COLOR_BGR2RGB)\nim=cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\nplt.imshow(im)\nh, w, c = im.shape\n\nnet_size = 12\n\ncurrent_scale = float(net_size) / 350# find initial scale\n# print('imgshape:{0}, current_scale:{1}'.format(im.shape, current_scale))\nim_resized = resize_image(im, current_scale) # scale = 1.0\ncurrent_height, current_width, _ = im_resized.shape\nk=changetotensor(im_resized)\ncls_map, reg = myOutputcls(mypnet.to(device),k.to(device)),myOutputBb(mypnet.to(device),k.to(device))\ncls_map=cls_map.permute(0,2,3,1)\nplt.imshow(F.sigmoid(cls_map).detach().cpu().numpy()[0]>0.240)\nplt.show()\nplt.imshow(im_resized)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:29:25.275771Z","iopub.execute_input":"2021-10-18T10:29:25.276584Z","iopub.status.idle":"2021-10-18T10:29:25.723508Z","shell.execute_reply.started":"2021-10-18T10:29:25.276533Z","shell.execute_reply":"2021-10-18T10:29:25.722653Z"},"trusted":true},"execution_count":248,"outputs":[]},{"cell_type":"code","source":"reg_bb=reg.permute(0,2,3,1)[0].detach().cpu().numpy()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:29:26.678673Z","iopub.execute_input":"2021-10-18T10:29:26.679552Z","iopub.status.idle":"2021-10-18T10:29:26.683853Z","shell.execute_reply.started":"2021-10-18T10:29:26.679497Z","shell.execute_reply":"2021-10-18T10:29:26.683225Z"},"trusted":true},"execution_count":249,"outputs":[]},{"cell_type":"code","source":"reg_bb","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:29:28.444444Z","iopub.execute_input":"2021-10-18T10:29:28.445303Z","iopub.status.idle":"2021-10-18T10:29:28.455704Z","shell.execute_reply.started":"2021-10-18T10:29:28.445260Z","shell.execute_reply":"2021-10-18T10:29:28.454298Z"},"trusted":true},"execution_count":250,"outputs":[]},{"cell_type":"code","source":"lon=np.array(np.where(F.sigmoid(cls_map).detach().cpu().numpy()[0]>0.85))","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:29:29.476024Z","iopub.execute_input":"2021-10-18T10:29:29.476334Z","iopub.status.idle":"2021-10-18T10:29:29.481970Z","shell.execute_reply.started":"2021-10-18T10:29:29.476300Z","shell.execute_reply":"2021-10-18T10:29:29.481079Z"},"trusted":true},"execution_count":251,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"lon","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:29:31.073402Z","iopub.execute_input":"2021-10-18T10:29:31.074314Z","iopub.status.idle":"2021-10-18T10:29:31.081027Z","shell.execute_reply.started":"2021-10-18T10:29:31.074261Z","shell.execute_reply":"2021-10-18T10:29:31.080170Z"},"trusted":true},"execution_count":252,"outputs":[]},{"cell_type":"code","source":"reg_bb.shape,current_scale","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:29:32.502996Z","iopub.execute_input":"2021-10-18T10:29:32.503449Z","iopub.status.idle":"2021-10-18T10:29:32.509677Z","shell.execute_reply.started":"2021-10-18T10:29:32.503416Z","shell.execute_reply":"2021-10-18T10:29:32.508859Z"},"trusted":true},"execution_count":253,"outputs":[]},{"cell_type":"code","source":"final_boxes=[]\nfor x in range(len(lon[0])):\n    cell_pos= lon[:,x]\n    coordinates=reg_bb[cell_pos[0],cell_pos[1],:]\n    real_ord=np.array([(2*cell_pos[1])/current_scale ,(2*cell_pos[0])/current_scale ,(2*cell_pos[1] +12)/current_scale , (2*cell_pos[0] + 12)/current_scale],dtype=int)\n    print(coordinates)\n    print(real_ord)\n    final_boxes.append(real_ord)\n    cv2.rectangle(im,(real_ord[0],real_ord[1]),(real_ord[2],real_ord[3]),(40,125,200),2)\n    plt.imshow(im)\n    plt.title(f\"From {x} Activation Map\")\n    plt.show()\n    \n    #input()\n    ","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:29:38.105539Z","iopub.execute_input":"2021-10-18T10:29:38.106232Z","iopub.status.idle":"2021-10-18T10:29:40.314128Z","shell.execute_reply.started":"2021-10-18T10:29:38.106178Z","shell.execute_reply":"2021-10-18T10:29:40.313212Z"},"trusted":true},"execution_count":254,"outputs":[]},{"cell_type":"code","source":"final_boxes=np.array(final_boxes)\nfinal_scores=F.sigmoid(cls_map)[0][lon]","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:29:46.149563Z","iopub.execute_input":"2021-10-18T10:29:46.150139Z","iopub.status.idle":"2021-10-18T10:29:46.154601Z","shell.execute_reply.started":"2021-10-18T10:29:46.150100Z","shell.execute_reply":"2021-10-18T10:29:46.153970Z"},"trusted":true},"execution_count":255,"outputs":[]},{"cell_type":"code","source":"import numpy as np\n\n\ndef torch_nms(dets,scores, thresh, mode=\"Union\"):\n    \"\"\"\n    greedily select boxes with high confidence\n    keep boxes overlap <= thresh\n    rule out overlap > thresh\n    :param dets: [[x1, y1, x2, y2 score]]\n    :param thresh: retain overlap <= thresh\n    :return: indexes to keep\n    \"\"\"\n    x1 = dets[:, 0]\n    y1 = dets[:, 1]\n    x2 = dets[:, 2]\n    y2 = dets[:, 3]\n    scores =scores\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n    print(order)\n    keep = []\n    while order.size > 0:\n        \n        i = order[0]\n        keep.append(i)\n        xx1 = np.maximum(x1[i], x1[order[1:]])\n        yy1 = np.maximum(y1[i], y1[order[1:]])\n        xx2 = np.minimum(x2[i], x2[order[1:]])\n        yy2 = np.minimum(y2[i], y2[order[1:]])\n        print(i)\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n        if mode == \"Union\":\n            ovr = inter / (areas[i] + areas[order[1:]] - inter)\n        elif mode == \"Minimum\":\n            ovr = inter / np.minimum(areas[i], areas[order[1:]])\n        print(ovr)\n        inds = np.where(ovr <= thresh)[0]\n        order = order[inds + 1]\n        print(\"Tjere\")\n\n    return keep","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:29:47.978144Z","iopub.execute_input":"2021-10-18T10:29:47.979119Z","iopub.status.idle":"2021-10-18T10:29:47.991594Z","shell.execute_reply.started":"2021-10-18T10:29:47.979076Z","shell.execute_reply":"2021-10-18T10:29:47.990705Z"},"trusted":true},"execution_count":256,"outputs":[]},{"cell_type":"code","source":"final_scores","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:29:49.169136Z","iopub.execute_input":"2021-10-18T10:29:49.170080Z","iopub.status.idle":"2021-10-18T10:29:49.176016Z","shell.execute_reply.started":"2021-10-18T10:29:49.170037Z","shell.execute_reply":"2021-10-18T10:29:49.175242Z"},"trusted":true},"execution_count":257,"outputs":[]},{"cell_type":"code","source":"to_keep_indexes=torch_nms(final_boxes,final_scores.detach().cpu().numpy(),0.4)","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:32:08.527016Z","iopub.execute_input":"2021-10-18T10:32:08.527438Z","iopub.status.idle":"2021-10-18T10:32:08.534630Z","shell.execute_reply.started":"2021-10-18T10:32:08.527407Z","shell.execute_reply":"2021-10-18T10:32:08.533455Z"},"trusted":true},"execution_count":275,"outputs":[]},{"cell_type":"code","source":"to_keep_indexes","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:32:08.732406Z","iopub.execute_input":"2021-10-18T10:32:08.732699Z","iopub.status.idle":"2021-10-18T10:32:08.738502Z","shell.execute_reply.started":"2021-10-18T10:32:08.732668Z","shell.execute_reply":"2021-10-18T10:32:08.737525Z"},"trusted":true},"execution_count":276,"outputs":[]},{"cell_type":"code","source":"im=cv2.imread(f'../input/wider-face-recognization/WIDER_train/WIDER_train/images/{train_df.iloc[ind,0]}',cv2.COLOR_BGR2RGB)\nim=cv2.cvtColor(im,cv2.COLOR_BGR2RGB)\nfor x in to_keep_indexes:\n    print()\n    cv2.rectangle(im,(final_boxes[x][0],final_boxes[x][1]),(final_boxes[x][2],final_boxes[x][3]),(100,5,200),2)\nplt.imshow(im)\nplt.title(f\"From {x} Activation Map\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:32:08.999583Z","iopub.execute_input":"2021-10-18T10:32:09.000433Z","iopub.status.idle":"2021-10-18T10:32:09.350424Z","shell.execute_reply.started":"2021-10-18T10:32:09.000389Z","shell.execute_reply":"2021-10-18T10:32:09.349353Z"},"trusted":true},"execution_count":277,"outputs":[]},{"cell_type":"code","source":"!zip -r -qq pnet_complete.zip ./pnet/*","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:36:21.923692Z","iopub.execute_input":"2021-10-18T10:36:21.924027Z","iopub.status.idle":"2021-10-18T10:39:14.694024Z","shell.execute_reply.started":"2021-10-18T10:36:21.923991Z","shell.execute_reply":"2021-10-18T10:39:14.692701Z"},"trusted":true},"execution_count":281,"outputs":[]},{"cell_type":"code","source":"!rm ./pnet/train/neg -r\n!rm ./pnet/train/pos -r","metadata":{"execution":{"iopub.status.busy":"2021-10-18T10:41:22.172780Z","iopub.execute_input":"2021-10-18T10:41:22.173234Z","iopub.status.idle":"2021-10-18T10:41:22.181202Z","shell.execute_reply.started":"2021-10-18T10:41:22.173190Z","shell.execute_reply":"2021-10-18T10:41:22.180105Z"},"trusted":true},"execution_count":282,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}